{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AfterWork Data Science: Hyperparameter Tuning with Python Justus",
      "provenance": [],
      "collapsed_sections": [
        "I4acj-OTOP82",
        "2jTFOxfaOd14",
        "0CrlFuI-VjsD",
        "QaKj_EYnVnJa",
        "m4f-HCCzcsFn",
        "S6xAx-PccsFq",
        "mIk0U5Aqfhbz",
        "X-RiKkKFOrVb",
        "A9y1H556gVW_",
        "W6ndQUsSizcy",
        "okCcLzc11Pdg",
        "1yNS9_mH1Pdk",
        "k4U0vmDV1PfP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxYyRzuYN6u7"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ABs5Pr5OOdM"
      },
      "source": [
        "# AfterWork Data Science: Hyperparameter Tuning with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4acj-OTOP82"
      },
      "source": [
        "### Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpvueFt9N2Wr"
      },
      "source": [
        "# We will start by running this cell which will import the necessary libraries\n",
        "# ---\n",
        "# \n",
        "import pandas as pd                # Pandas for data manipulation\n",
        "import numpy as np                 # Numpy for scientific computation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jTFOxfaOd14"
      },
      "source": [
        "## 1. Manual Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrlFuI-VjsD"
      },
      "source": [
        "### Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtTRKfhiMyR"
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr4jsDQ7UgDk"
      },
      "source": [
        "# Step 1\n",
        "# ---\n",
        "# Loading our dataset \n",
        "social_df = pd.read_csv('http://bit.ly/SocialNetworkAdsDataset') \n",
        "\n",
        "# Data preparation: Encoding\n",
        "social_df[\"Gender\"] = np.where(social_df[\"Gender\"].str.contains(\"Male\", \"Female\"), 1, 0) \n",
        "\n",
        "# Defining our predictor and label variable\n",
        "X = social_df.iloc[:, [1, 2 ,3]].values  # Independent/predictor variables\n",
        "y = social_df.iloc[:, 4].values          # Dependent/label variable\n",
        "\n",
        "# Splitting our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "\n",
        "# Performing scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler() \n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAjFNtLnOgJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af34e0d7-c3a7-4885-941e-ec3fe0ad1b8e"
      },
      "source": [
        "# Step 2\n",
        "# ---\n",
        "# Defining our classifier\n",
        "from sklearn.tree import DecisionTreeClassifier  \n",
        "\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# We will focus on two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "#\n",
        "decision_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fitting our data\n",
        "decision_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSqqtM59QxMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2171805f-8211-4da5-82d5-927525c91669"
      },
      "source": [
        "# Step 3\n",
        "# ---\n",
        "# Making our predictions\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "\n",
        "# Calculating our metrics\n",
        "# ---\n",
        "#\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
        "print(accuracy_score(decision_y_prediction, y_test))\n",
        "print(confusion_matrix(decision_y_prediction, y_test))\n",
        "print(classification_report(decision_y_prediction, y_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.84\n",
            "[[56  9]\n",
            " [ 7 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.88        65\n",
            "           1       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.84       100\n",
            "   macro avg       0.82      0.83      0.83       100\n",
            "weighted avg       0.84      0.84      0.84       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE20z-jvT69F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc5c583-22eb-4d89-ba6c-d5ec8ed99fc4"
      },
      "source": [
        "# Repeating Step 2\n",
        "# ---\n",
        "# Let's now perform hyper parameter tuning by setting \n",
        "# the hyperparameters max_depth = 2 and min_samples_leaf = 100\n",
        "# and get our output?\n",
        "# ---\n",
        "# \n",
        "decision_classifier = DecisionTreeClassifier(max_depth = 2, min_samples_leaf = 50, random_state=42)\n",
        "\n",
        "# Fitting our data\n",
        "decision_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=2, min_samples_leaf=50, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rygoolwDUzJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45eec6c-4d53-4c2a-b634-17dca54027e1"
      },
      "source": [
        "# Repeating Step 3\n",
        "# --- \n",
        "# Step 3\n",
        "# ---\n",
        "# Making our predictions\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        " \n",
        "# Calculating our metrics \n",
        "print(accuracy_score(decision_y_prediction, y_test))\n",
        "print(confusion_matrix(decision_y_prediction, y_test))\n",
        "print(classification_report(decision_y_prediction, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9\n",
            "[[54  1]\n",
            " [ 9 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.98      0.92        55\n",
            "           1       0.97      0.80      0.88        45\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.92      0.89      0.90       100\n",
            "weighted avg       0.91      0.90      0.90       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KQIlwybVZtn"
      },
      "source": [
        "Can you get a better accuracy? By tuning the same hyperparameters or other parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJD2CgDZXVNj"
      },
      "source": [
        "To read more about hyper parameter tuning for decision trees, you can refer to this reading: [Link](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKj_EYnVnJa"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWOOvFmVvHc"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Using the given dataset above, create a logistic regression classifier \n",
        "# then tune its hyperparameters to get the best possible accuracy.\n",
        "# Make a comparisons of your with other fellows in your breakout rooms.\n",
        "# Hint: Use the following documentation to tune the hyper parameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4f-HCCzcsFn"
      },
      "source": [
        "## 2. Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6xAx-PccsFq"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFnMcdWliR-E"
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jotViGxKb8Yp"
      },
      "source": [
        "# Step 2\n",
        "# ---\n",
        "# Defining our classifier \n",
        "\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# Again we will focus on the same two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "# \n",
        "decision_classifier = DecisionTreeClassifier(random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q03AxVzIZprI"
      },
      "source": [
        "# Step 3: Hyperparameters: Getting Started with Grid Search\n",
        "# ---\n",
        "# We will continue from where we left off from the previous example,\n",
        "# We will create a dictionary of all the parameters and their corresponding \n",
        "# set of values that you want to test for best performance. \n",
        "# The name of the dictionary items corresponds to the parameter name \n",
        "# and the value corresponds to the list of values for the parameter.\n",
        "# As shown grid_param dictionary with two parameters max_depth, min_samples_leaf.\n",
        "# The parameter values that we want to try out are passed in the list.   \n",
        "# The Grid Search algorithm basically would check for all possible combinations \n",
        "# of parameter values and returns the combination with the best accuracy. \n",
        "# For instance, in the above case the Grid Search algorithm \n",
        "# will check all combinations (5 x 5 = 25).\n",
        "# ---\n",
        "# \n",
        "grid_param = {\n",
        "    'max_depth': [2, 3, 4, 10, 15],\n",
        "    'min_samples_leaf': [10, 20, 30, 40, 50]\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgzqZY71Z2Vv"
      },
      "source": [
        "# Step 2: Instantiating GridSearchCV object\n",
        "# ---\n",
        "# We then create an instance of the GridSearchCV class \n",
        "# and pass values for the estimator parameter, \n",
        "# which basically is the algorithm that you want to execute. \n",
        "# The param_grid parameter takes the created grid dictionary \n",
        "# The scoring parameter takes the performance metrics, \n",
        "# the cv parameter corresponds to number of folds, which will set 5 in our case, \n",
        "# and finally the n_jobs parameter refers to the number of CPU's that we want to use for execution. \n",
        "# Setting the value of n_jobs = -1 allows us us to use all available computing power.\n",
        "# You can refer to the GridSearchCV documentation to find out more: https://bit.ly/2Yr0qVC\n",
        "# ---\n",
        "# \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gd_sr_cl = GridSearchCV(estimator = decision_classifier,\n",
        "                     param_grid = grid_param,\n",
        "                     scoring = 'accuracy',\n",
        "                     cv = 5,\n",
        "                     n_jobs =-1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xurXa_ovZ5JE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323d5ab9-ccbf-48aa-aac9-53ad854daa68"
      },
      "source": [
        "# Step 3: Calling the fit method\n",
        "# ---\n",
        "# We now fit our data and call the fit method of the class \n",
        "# and pass it the training and test set, as shown in the following code.\n",
        "# If we had lost of other parameters this would take abit of some time to execute. \n",
        "# This is because the GridSearchCV would go through all the combinations of hyperparameters. \n",
        "# ---\n",
        "# \n",
        "gd_sr_cl.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={'max_depth': [2, 3, 4, 10, 15],\n",
              "                         'min_samples_leaf': [10, 20, 30, 40, 50]},\n",
              "             scoring='accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSjIyP6iZ7gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9818281-10eb-481a-e2ba-3647e9f515df"
      },
      "source": [
        "# Step 4\n",
        "# --- \n",
        "# We use gd_sr_cl.best_params_ attribute of the GridSearchCV object\n",
        "# to check the parameters with the highest accuracy\n",
        "# ---\n",
        "# \n",
        "best_parameters = gd_sr_cl.best_params_\n",
        "print(best_parameters)\n",
        " \n",
        "# We shouldn't stop here however, instead we should add  \n",
        "# other estimators and see if the accuracy increases  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 3, 'min_samples_leaf': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY8IpIK1Z9gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769a9d8b-1e2e-4e34-c299-22b57ddfce6b"
      },
      "source": [
        "# Step 5: Finding the obtained accuracy\n",
        "# ---\n",
        "# We can also obtain the best accuracy by doing the following\n",
        "# ---\n",
        "# \n",
        "best_result = gd_sr_cl.best_score_\n",
        "print(best_result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9066666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NZOSpZBc7CU"
      },
      "source": [
        "Can you get a better accuracy? By refering to the decision tree documentation, choosing additional approriate hyper-parameters and set the hyperparameter values to the grid search space in an effort to get a better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIk0U5Aqfhbz"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEMGtQlpfk8c"
      },
      "source": [
        "# Challenge\n",
        "# ---\n",
        "# In this challenge, we still be required to use grid search while using \n",
        "# the logistic regression classifier we created earlier to get the best possible accuracy. \n",
        "# Hint: Use the following documentation to tune the hyperparameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-RiKkKFOrVb"
      },
      "source": [
        "## 3. Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9y1H556gVW_"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPGxiGQFgVXB"
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvOuYIxW8SaB"
      },
      "source": [
        "# Defining our classifier \n",
        "# ---\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# Again, we will focus on the same two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "# \n",
        "decision_classifier = DecisionTreeClassifier(random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTlhWCWglW_"
      },
      "source": [
        "# Step 1: Hyperparameters: Getting Started with Random Search\n",
        "# ---\n",
        "# While performing random search, we would need to provide a statistical distribution \n",
        "# for each hyperparameter from which values may be randomly sampled.\n",
        "# We'll define a sampling distribution for each hyperparameter.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# Let's define our parameters and the respective distributions to sample from\n",
        "# ---\n",
        "#\n",
        "from scipy.stats import randint as sp_randint\n",
        "param_dist = {\"max_depth\": [3, None], \n",
        "              \"min_samples_leaf\": sp_randint(1, 50)}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDyEIalgmiN"
      },
      "source": [
        "# Step 2\n",
        "# ---\n",
        "# We then instantiate our RandomizedSearchCV object \n",
        "# ---\n",
        "# We can read more about the RandomizedSearchCV documentation\n",
        "# by following this link: https://bit.ly/2V9Xhri\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "random_sr = RandomizedSearchCV(decision_classifier, param_dist, cv = 5) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mVVunKJgrMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6e20e8-921d-46fa-a51a-206589ccfae4"
      },
      "source": [
        "# Step 3: Then fitting our data\n",
        "# ---\n",
        "#\n",
        "random_sr.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
              "                   param_distributions={'max_depth': [3, None],\n",
              "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fdd402ac990>})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4v1u9vVguNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2091d1a4-387a-4ec2-aaee-56767ee31f6d"
      },
      "source": [
        "# Step 4: Checking for the best parameters\n",
        "# ---\n",
        "#\n",
        "best_parameters = random_sr.best_params_\n",
        "print(best_parameters)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': None, 'min_samples_leaf': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzKzEqLxgvx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4536b787-3950-4082-cc0d-097d33d73752"
      },
      "source": [
        "# And lastly obtaining our accuracy\n",
        "# --\n",
        "# \n",
        "best_result = random_sr.best_score_\n",
        "print(best_result)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9066666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx4Qs-8rjUQu"
      },
      "source": [
        "Can you get a better accuracy? By refering to the decision tree documentation, choosing additional approriate hyper-parameters and set the hyperparameter values to the random search space in an effort to get a better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6ndQUsSizcy"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taJUBjUJizc0"
      },
      "source": [
        "# Challenge\n",
        "# ---\n",
        "# Again, we will also be required to use random search while using \n",
        "# the logistic regression classifier we created earlier to get the best possible accuracy. \n",
        "# Hint: Use the following documentation to tune the hyperparameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okCcLzc11Pdg"
      },
      "source": [
        "## 4. Bayesian Optimisation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNS9_mH1Pdk"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ6F9fmA1Pdq"
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R72DljV7mWb"
      },
      "source": [
        "# Defining our classifier \n",
        "# ---\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# Again, we will focus on the same two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "# "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "havuVOJ81Pd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c2817be7-4694-417b-c4f5-79a8d4746438"
      },
      "source": [
        "# Step 1: Hyperparameters: Getting Started with Bayesian Optimisation\n",
        "# ---\n",
        "# While performing bayesian optimisation, we perform the following steps, \n",
        "# 1. Set up a space dictionary.\n",
        "# - In this space, we create a probability distribution for each of the used hyperparameters.\n",
        "# 2. Set up the objective function using the respective classifier/regressor.\n",
        "# 3. Run our Bayesian Optimizer.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# Let's define set up our space dictionary \n",
        "# ---\n",
        "#\n",
        "\n",
        "# We will import the hyperopt library which will helps us perform bayesian optimisation\n",
        "# ---\n",
        "# Hyperopt librariy documentation: https://bit.ly/2Dyynf4\n",
        "# ---\n",
        "#\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 1. Setting up a our space dictionary\n",
        "# ---\n",
        "#\n",
        "space = {'max_depth': hp.quniform('max_depth', 10, 1200, 10), \n",
        "        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5)}\n",
        "\n",
        "# 2. Setting up our objective function\n",
        "# ----\n",
        "#\n",
        "def objective(space): \n",
        "    classifier = DecisionTreeClassifier(max_depth = space['max_depth'],\n",
        "                                 min_samples_leaf = space['min_samples_leaf'])\n",
        "    \n",
        "    accuracy = cross_val_score(classifier, X_train, y_train, cv = 4).mean() \n",
        "\n",
        "    # We aim to maximize accuracy; in this case we return it as a negative value\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
        "\n",
        "# 3. Running our bayesian optimizer\n",
        "# ---\n",
        "#\n",
        "best = fmin(fn= objective,                        # the objective function to miminize / the loss function to minimize\n",
        "            space = space,                        # the range of input values to test during optimisation\n",
        "            algo= tpe.suggest,                    # the search algorithm to use\n",
        "            max_evals = 80,                       # the no. of iteration to perform\n",
        "            rstate=np.random.RandomState(42))     # the randomstate for reproducability / to get the same result when we run the code\n",
        "\n",
        "# printing out our outcome\n",
        "best"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a4e39adf4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0;31m# the search algorithm to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# the no. of iteration to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             rstate=np.random.RandomState(42))     # the randomstate for reproducability / to get the same result when we run the code\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# printing out our outcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opu_jYhf_0Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52a3977-5bde-498b-e7a1-bb4b2617b481"
      },
      "source": [
        "# We can access values of the above paramemters by\n",
        "# ---\n",
        "#\n",
        "print(\"Max Depth:\", best['max_depth'])\n",
        "print(\"Min Samples Leaf:\", best['min_samples_leaf'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: 630.0\n",
            "Min Samples Leaf: 0.08629075344668666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jJPl9FH5YiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31527bb-3fcc-4083-9062-fb3c249cc3bc"
      },
      "source": [
        "# Let's now perform our classification with our optimal hyperparameters  \n",
        "# ---\n",
        "# \n",
        "decision_classifier = DecisionTreeClassifier(max_depth = best['max_depth'], \n",
        "                                             min_samples_leaf = best['min_samples_leaf'], random_state=42)\n",
        "\n",
        "# Fitting our data\n",
        "decision_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=630.0, min_samples_leaf=0.08629075344668666,\n",
              "                       random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSHEGoUL6I_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a94c4e1-009c-40ee-d219-a9c123f78031"
      },
      "source": [
        "# Making our predictions\n",
        "# ---\n",
        "#\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "\n",
        "# Calculating our metrics \n",
        "print(accuracy_score(decision_y_prediction, y_test))\n",
        "print(confusion_matrix(decision_y_prediction, y_test))\n",
        "print(classification_report(decision_y_prediction, y_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.92\n",
            "[[57  2]\n",
            " [ 6 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.93        59\n",
            "           1       0.95      0.85      0.90        41\n",
            "\n",
            "    accuracy                           0.92       100\n",
            "   macro avg       0.93      0.91      0.92       100\n",
            "weighted avg       0.92      0.92      0.92       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPAUxzjx1PfO"
      },
      "source": [
        "Can you get a better accuracy? By refering to the decision tree documentation, choosing additional approriate hyper-parameters and set the hyperparameter values to the  search space in an effort to get a better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4U0vmDV1PfP"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q5dXaLj1PfP"
      },
      "source": [
        "# Challenge\n",
        "# ---\n",
        "# Again, we will also be required to use bayesian optimisation while using \n",
        "# the logistic regression classifier we created earlier to get the best possible accuracy. \n",
        "# Hint: Use the following documentation to tune the hyperparameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}